<!DOCTYPE html>
<html>
  <head>
    <title>CSSS508, Week 11</title>
    <meta charset="utf-8">
    <meta name="author" content="Chuck Lanfear" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, title-slide

# CSSS508, Week 11
## Working with Model Results
### Chuck Lanfear
### Dec 5, 2018<br>Updated: Dec 3, 2018

---









# Topics for Today

Displaying Model Results

* `broom`
   + Turning model output lists into dataframes
   + Summarizing models
* `ggeffects`
   + Creating counterfactual estimates
   + Plotting marginal effects
* Manual counterfactual plots
* Making regression tables
   + Using `pander` for models
   + Using `sjTable()` in `sjPlot`
* Wrapping up the course

---
class: inverse
# `broom`

---
# `broom`

`broom` is a package that "tidies up" the output from models such a `lm()` and `glm()`.

It has a small number of key functions:

* `tidy()` - Creates a dataframe summary of a model.
* `augment()` - Adds columns—such as fitted values—to the data used in the model.
* `glance()` - Provides one row of fit statistics for models.


```r
library(broom)
```

---
# Model Output is a List

`lm()` and `summary()` produce lists as output, which cannot go directly into 
tidyverse functions, particularly those in `ggplot2`.

.small[

```r
lm_1 &lt;- lm(yn~x1+x2, data=ex_dat)
summary(lm_1)
```

```
## 
## Call:
## lm(formula = yn ~ x1 + x2, data = ex_dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.403 -1.736 -0.251  1.920  8.126 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.0228     0.4151   2.464 0.014593 *  
## x1            0.5205     0.1015   5.128 7.01e-07 ***
## x2B           1.3096     0.5192   2.522 0.012453 *  
## x2C           1.8982     0.5420   3.502 0.000572 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.903 on 196 degrees of freedom
## Multiple R-squared:  0.1797,	Adjusted R-squared:  0.1672 
## F-statistic: 14.32 on 3 and 196 DF,  p-value: 1.796e-08
....
```
]

---
# Model Output Varies!

.smallish[
Each type of model also produces somewhat different output, so you can't just reuse
the same code to handle output from every model.
]

.small[

```r
glm_1 &lt;- glm(yb~x1+x2, data=ex_dat, family=binomial(link="logit"))
summary(glm_1)
```

```
## 
## Call:
## glm(formula = yb ~ x1 + x2, family = binomial(link = "logit"), 
##     data = ex_dat)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6858  -1.1167  -0.5858   1.1152   1.8117  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.72146    0.31058  -2.323 0.020179 *  
## x1           0.29742    0.07853   3.787 0.000152 ***
## x2B          0.35743    0.37776   0.946 0.344056    
## x2C          0.60670    0.39431   1.539 0.123897    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
....
```
]

---
# `broom::tidy()`

`tidy()` produces the similar output, but as a dataframe.


```r
lm_1 %&gt;% tidy()
```

```
## # A tibble: 4 x 5
##   term        estimate std.error statistic     p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 (Intercept)    1.02      0.415      2.46 0.0146     
## 2 x1             0.521     0.102      5.13 0.000000701
## 3 x2B            1.31      0.519      2.52 0.0125     
## 4 x2C            1.90      0.542      3.50 0.000572
```

Each type of model (e.g. `glm`, `lmer`) has a different *method* with its own additional arguments. See `?tidy.lm` for an example.

---
# `broom::tidy()`

This output is also completely identical between different models.

This can be very 
useful and important if running models with different test statistics... or just running
a lot of models!


```r
glm_1 %&gt;% tidy()
```

```
## # A tibble: 4 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   -0.721    0.311     -2.32  0.0202  
## 2 x1             0.297    0.0785     3.79  0.000152
## 3 x2B            0.357    0.378      0.946 0.344   
## 4 x2C            0.607    0.394      1.54  0.124
```

---
# `broom::glance()`

`glance()` produces dataframes of fit statistics for models.

If you run many models,
you can compare each model row-by-row in each column... or even plot their different
fit statistics to allow holistic comparison.

.small[

```r
glance(lm_1)
```

```
## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC
## *     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.180         0.167  2.90      14.3 1.80e-8     4  -495. 1000. 1016.
## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;
```
]

---
# `broom augment()`

`augment()` takes values generated by a model and adds them back to the original data.
This includes fitted values, residuals, and leverage statistics.

.small[

```r
augment(lm_1) %&gt;% head()
```

```
## # A tibble: 6 x 10
##        yn     x1 x2    .fitted .se.fit .resid   .hat .sigma .cooksd
##     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
## 1  3.18   -1.61  B       1.49    0.431  1.68  0.0220   2.91 1.94e-3
## 2  7.45    1.27  B       3.00    0.321  4.46  0.0122   2.89 7.36e-3
## 3  0.0650  0.169 A       1.11    0.412 -1.05  0.0201   2.91 6.80e-4
## 4  2.18    3.33  C       4.65    0.407 -2.48  0.0197   2.90 3.73e-3
## 5 -1.48   -0.343 A       0.844   0.423 -2.32  0.0213   2.91 3.56e-3
## 6  5.60    4.77  B       4.82    0.482  0.785 0.0276   2.91 5.34e-4
## # ... with 1 more variable: .std.resid &lt;dbl&gt;
```
]

---
# The Power of `broom`

The real advantage of `broom` becomes apparent when running many models at once. Here we run separate models for each level of `x2`:

.small[

```r
*ex_dat %&gt;% group_by(x2) %&gt;% do(tidy(lm(yn ~  x1 + x3 + x4, data=ex_dat)))
```

```
## # A tibble: 12 x 6
## # Groups:   x2 [3]
##    x2    term        estimate std.error statistic  p.value
##    &lt;fct&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 A     (Intercept)    1.61     0.217       7.42 3.46e-12
##  2 A     x1             0.564    0.0694      8.12 5.09e-14
##  3 A     x3             0.690    0.0449     15.4  1.83e-35
##  4 A     x4No           0.984    0.282       3.49 6.02e- 4
##  5 B     (Intercept)    1.61     0.217       7.42 3.46e-12
##  6 B     x1             0.564    0.0694      8.12 5.09e-14
##  7 B     x3             0.690    0.0449     15.4  1.83e-35
##  8 B     x4No           0.984    0.282       3.49 6.02e- 4
##  9 C     (Intercept)    1.61     0.217       7.42 3.46e-12
## 10 C     x1             0.564    0.0694      8.12 5.09e-14
## 11 C     x3             0.690    0.0449     15.4  1.83e-35
## 12 C     x4No           0.984    0.282       3.49 6.02e- 4
```
]

.footnote[`do()` repeats whatever is inside it once for each level of the variable(s) in `group_by()` then puts them together as a data frame.]
---
class: inverse
# `ggeffects`

---
# But first, vocab!

We are often interested in what might happen if some variables take particular values, often ones not seen in the actual data.

--

When we set variables to certain values, we refer to them as **counterfactual values** or just **counterfactuals**.

--

For example, if we know nothing about a new observation, our prediction for that estimate is often based on assuming every variable is at its mean.

--

Sometimes, however, we might have very specific questions which require setting (possibly many) combinations of variables to particular values and making an estimate or prediction.

--

Providing specific estimates, conditional on values of covariates, is a nice way to summarize results, particularly for models with unintuitive parameters (e.g. logit models).

---
# `ggeffects`

While `broom` produces tidy model *summaries*, `ggeffects` is used to create tidy 
*marginal effects*.

That is, tidy dataframes of *ranges* of predicted values that can be
fed straight into `ggplot2` for plotting model results.

We will focus on two `ggeffects` functions:

* `ggpredict()` - Computes predicted values for the outcome variable at margins of specific variables.
* `plot.ggeffects()` - A plot method for `ggeffects` objects (like `ggredict()` output)


```r
library(ggeffects)
```

---
# `ggpredict()`

When you run  `ggpredict()`, it produces a dataframe with a row for every unique 
value of a supplied predictor ("independent") variable (`term`). 

Each row contains an expected (estimated) value for the outcome ("dependent") variable, plus confidence intervals. 


```r
lm_1_est&lt;- ggpredict(lm_1, terms="x1")
```

If desired, the argument `interval="prediction"` will give predicted intervals instead.

---
#`ggpredict()` output

.smallish[

```r
lm_1_est
```

```
## 
## # Predicted values for yn 
## # x = x1 
## 
##   x predicted conf.low conf.high
##  -4    -1.059   -2.307     0.188
##  -2    -0.018   -0.994     0.957
##   0     1.023    0.209     1.836
##   2     2.064    1.234     2.894
##   4     3.105    2.089     4.121
##   6     4.146    2.845     5.448
##   8     5.187    3.553     6.822
## 
## Adjusted for:
## * x2 = A
```
]

---
# `plot()` for `ggpredict()`

`ggeffects` features a `plot()` *method*, `plot.ggeffects()`, which produces
a ggplot when you give `plot()` output from `ggpredict()`.

.small[

```r
plot(lm_1_est)
```

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-12-1.svg)&lt;!-- --&gt;
]

---
# Grouping with `ggpredict()`

When using a vector of `terms`, `ggeffects` will plot the first along the x-axis and use
others for *grouping*. Note we can pipe a model into `ggpredict()`!

.small[

```r
glm(yb ~ x1 + x2 + x3 + x4, data=ex_dat, family=binomial(link="logit")) %&gt;%
  ggpredict(terms=c("x1", "x2")) %&gt;% plot()
```

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-13-1.svg)&lt;!-- --&gt;
]

---
# Faceting with `ggpredict()`

You can add `facet=TRUE` to the `plot()` call to facet over *grouping terms*.

.small[

```r
glm(yb ~ x1 + x2 + x3 + x4, data=ex_dat, family=binomial(link="logit")) %&gt;%
  ggpredict(terms=c("x1", "x2")) %&gt;% plot(facet=TRUE)
```

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-14-1.svg)&lt;!-- --&gt;
]

---
# Counterfactual Values

You can add values in square brackets in the `terms=` argument to specify counterfactual values.

.small[

```r
glm(yb ~ x1 + x2 + x3 + x4, data=ex_dat, family=binomial(link="logit")) %&gt;%
  ggpredict(terms=c("x1 [-1,0,1]", "x2 [A,B]")) %&gt;% plot(facet=TRUE)
```

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-15-1.svg)&lt;!-- --&gt;
]

---
# Representative Values

You can also use `[meansd]` or `[minmax]` to set representative values.

.small[

```r
glm(yb ~ x1 + x2 + x3 + x4, data=ex_dat, family=binomial(link="logit")) %&gt;%
  ggpredict(terms=c("x1 [meansd]", "x3 [minmax]")) %&gt;% plot(facet=TRUE)
```

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-16-1.svg)&lt;!-- --&gt;
]

---
# Dot plots with `ggpredict()`

`ggpredict` will produce dot plots with error bars for categorical predictors.

.small[

```r
lm(yn ~ x2 + x4, data=ex_dat) %&gt;% 
  ggpredict(terms=c("x2", "x4")) %&gt;% plot()
```

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-17-1.svg)&lt;!-- --&gt;
]

---
# Notes on `ggeffects`

There is a lot more to the `ggeffects` package that you can see in [the package vignette](https://cran.r-project.org/web/packages/ggeffects/vignettes/marginaleffects.html)
and the [github repository](https://github.com/strengejacke/ggeffects). This includes,
but is not limited to:

* Predicted values for polynomial and interaction terms

* Getting predictions from models from dozens of other packages

* Sending `ggeffects` objects to `ggplot2` to freely modify plots

---
# An Advanced Example

Here is an example using a model from a [recent article I worked on](https://onlinelibrary.wiley.com/doi/abs/10.1111/cico.12346?af=R).

This models the likelihood of arrest of a target in a police contact conditional on neighborhood, race of target, and race of who called the police.

.smallish[

```r
load("data/any_arrest_data.RData")
mod_arrest &lt;- glm(arrest ~ white_comp_wit_vict*black_arr_susp + 
                  crime_type*white_comp_wit_vict + caller_type + 
                  arr_susp_subj_count + comp_wit_vict_count +
                  black_arr_susp*neighb_type + crime_type*neighb_type + 
                  serious_rate + pbl + pot + dis + year,
                  family = binomial(link = "logit"),
                  data = any_arrest_data)
```
]

There are a lot of interactions here:

* Target Race x Caller Race
* Crime Type x Caller Race
* Target Race x Neigbhorhood Type
* Crime Type x Neighborhood Type

---
# `ggeffects` Output

.smallish[

```r
mod_arrest %&gt;% ggpredict(terms=c("black_arr_susp", 
              "white_comp_wit_vict", "neighb_type")) %&gt;% plot()
```

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-19-1.svg)&lt;!-- --&gt;
]

---
# A Complex Example

`ggpredict()` can only handle three variables in its `terms=` argument.

--

For my article, I wanted to plot estimates across counterfactual values of all four variables in my interaction terms:

* Caller Race
* Target Race
* Crime Type
* Neighborhood Type

How could I do this?

--

Stats + Math + Code = `\(\heartsuit\)`

---
# Some Background

Given we've estimate a model, consider the following:

1. `\(\hat{Y} = X\hat{\beta}\)`, where `\(X\)` is the model matrix and `\(\hat{\beta}\)` is the coefficients.
2. `\(\hat{\beta}\)` is a vector of *random variables* whose estimated distribution is described by parameter variance-covariance matrix `\(\Sigma\)`.

--

Using this, we can do the following:

1. Extract the model matrix `\(X\)`, estimated coefficients ( `\(\hat{\beta}\)` ), and `\(\Sigma\)` from our fitted model.
2. Make lots of random parameter draws centered on `\(\hat{\beta}\)` and distributed according to `\(\Sigma\)`.
3. Multiply *each* of these draws by *counterfactual* `\(X\)` *values* to get `\(\hat{Y}\)` values.
4. Take the 2.5% and 97.% quantiles of these `\(\hat{Y}\)` values.

--

This produces a *simulated* mean and confidence interval. This is called the **percentile method**, a type of *bootstrapping*.

---
# Simulating Coefficients

We can make random draws from our estimated distribution of parameters using `MASS::mvrnorm()` which takes three main arguments:
1. `n`: The number of draws
2. `mu`: mean—our coefficient estimates—obtained via `coef()`.
3. `Sigma`: a covariance matrix, obtained via `vcov()`.

.smallish[

```r
sim_params &lt;- MASS::mvrnorm(n=10000, 
                            mu=coef(mod_arrest),
                            Sigma=vcov(mod_arrest))
sim_params[1:6, 1:4]
```

```
##      (Intercept) white_comp_wit_vict1 black_arr_susp1 crime_typeNuisance
## [1,]    2.588985           -0.1824904      -0.1909731         -0.4362330
## [2,]    2.876980           -0.3423833      -0.3469296         -0.4920686
## [3,]    2.895096           -0.3223082      -0.4528482         -0.5534518
## [4,]    2.544341           -0.1316294      -0.2077112         -0.5963321
## [5,]    2.647856           -0.1552346      -0.2005195         -0.4578715
## [6,]    2.710290           -0.2878294      -0.3440114         -0.5908317
```
]

---
# Counterfactual Values

Next we need a data frame with our counterfactual values.

We want one row (or *scenario*) per estimate to plot, and all variables at their means *except* the ones we are varying. We also don't want impossible values; `neighb_type` values are mutually exclusive.



.smallish[

```r
x_values &lt;- colMeans(model.matrix(mod_arrest)) # vars at mean
n_scen   &lt;- (2*2*2*3) # Number of scenarios
x_frame  &lt;- setNames(data.frame(matrix(x_values, nrow=n_scen, 
                                       ncol=length(x_values), 
                                       byrow=T)), names(x_values))
*cf_vals  &lt;- arrangements::permutations(c(0,1), k=5, replace=T)
cf_vals  &lt;- cf_vals[cf_vals[,4]+cf_vals[,5]!=2 ,] # Remove impossible vals
colnames(cf_vals) &lt;- c("white_comp_wit_vict1", "black_arr_susp1", 
                       "crime_typeNuisance", "neighb_typeBlackDisadv",
                       "neighb_typeChanging")
x_frame[colnames(cf_vals)] &lt;- cf_vals # assign to countefactual df
```
]

.footnote[`permutations()` is a quick way to get all combinations of some values.]

---
# What Do We Have?

.small[

```r
glimpse(x_frame)
```

```
## Observations: 24
## Variables: 24
## $ `(Intercept)`                               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, ...
## $ white_comp_wit_vict1                        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, ...
## $ black_arr_susp1                             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, ...
## $ crime_typeNuisance                          &lt;dbl&gt; 0, 0, 0, 1, 1, 1, ...
## $ caller_typeVictim                           &lt;dbl&gt; 0.8019442, 0.80194...
## $ caller_typeWitness                          &lt;dbl&gt; 0.08516245, 0.0851...
## $ arr_susp_subj_count                         &lt;dbl&gt; 1.552955, 1.552955...
## $ comp_wit_vict_count                         &lt;dbl&gt; 1.571604, 1.571604...
## $ neighb_typeBlackDisadv                      &lt;dbl&gt; 0, 0, 1, 0, 0, 1, ...
## $ neighb_typeChanging                         &lt;dbl&gt; 0, 1, 0, 0, 1, 0, ...
## $ serious_rate                                &lt;dbl&gt; 2.157661e-17, 2.15...
## $ pbl                                         &lt;dbl&gt; 1.185962e-16, 1.18...
## $ pot                                         &lt;dbl&gt; -2.808814e-17, -2....
## $ dis                                         &lt;dbl&gt; 9.987197e-18, 9.98...
## $ year2009                                    &lt;dbl&gt; 0.2829368, 0.28293...
## $ year2010                                    &lt;dbl&gt; 0.1670504, 0.16705...
## $ year2011                                    &lt;dbl&gt; 0.09201842, 0.0920...
## $ year2012                                    &lt;dbl&gt; 0.1232284, 0.12322...
## $ `white_comp_wit_vict1:black_arr_susp1`      &lt;dbl&gt; 0.4452034, 0.44520...
## $ `white_comp_wit_vict1:crime_typeNuisance`   &lt;dbl&gt; 0.1614479, 0.16144...
## $ `black_arr_susp1:neighb_typeBlackDisadv`    &lt;dbl&gt; 0.04893835, 0.0489...
## $ `black_arr_susp1:neighb_typeChanging`       &lt;dbl&gt; 0.111691, 0.111691...
## $ `crime_typeNuisance:neighb_typeBlackDisadv` &lt;dbl&gt; 0.0154771, 0.01547...
## $ `crime_typeNuisance:neighb_typeChanging`    &lt;dbl&gt; 0.03300077, 0.0330...
```
]

---
# Fixing Interactions

Our main variables are correct... but we need to make our interaction terms.

The interaction terms in the model matrix have specific form `var1:var2`.

Their counterfactual values are just equal to the products of their components.

.small[

```r
x_frame &lt;- x_frame %&gt;%
 mutate(
  `white_comp_wit_vict1:black_arr_susp1`      = white_comp_wit_vict1*black_arr_susp1,
  `white_comp_wit_vict1:crime_typeNuisance`   = white_comp_wit_vict1*crime_typeNuisance,
  `black_arr_susp1:neighb_typeBlackDisadv`    = black_arr_susp1*neighb_typeBlackDisadv,
  `black_arr_susp1:neighb_typeChanging`       = black_arr_susp1*neighb_typeChanging,
  `crime_typeNuisance:neighb_typeBlackDisadv` = crime_typeNuisance*neighb_typeBlackDisadv,
  `crime_typeNuisance:neighb_typeChanging`    = crime_typeNuisance*neighb_typeChanging,
  `black_arr_susp1:neighb_typeBlackDisadv`    = black_arr_susp1*neighb_typeBlackDisadv,
  `black_arr_susp1:neighb_typeChanging`       = black_arr_susp1*neighb_typeChanging)
```
]

---
# Fixed

.small[

```r
glimpse(x_frame)
```

```
## Observations: 24
## Variables: 24
## $ `(Intercept)`                               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, ...
## $ white_comp_wit_vict1                        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, ...
## $ black_arr_susp1                             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, ...
## $ crime_typeNuisance                          &lt;dbl&gt; 0, 0, 0, 1, 1, 1, ...
## $ caller_typeVictim                           &lt;dbl&gt; 0.8019442, 0.80194...
## $ caller_typeWitness                          &lt;dbl&gt; 0.08516245, 0.0851...
## $ arr_susp_subj_count                         &lt;dbl&gt; 1.552955, 1.552955...
## $ comp_wit_vict_count                         &lt;dbl&gt; 1.571604, 1.571604...
## $ neighb_typeBlackDisadv                      &lt;dbl&gt; 0, 0, 1, 0, 0, 1, ...
## $ neighb_typeChanging                         &lt;dbl&gt; 0, 1, 0, 0, 1, 0, ...
## $ serious_rate                                &lt;dbl&gt; 2.157661e-17, 2.15...
## $ pbl                                         &lt;dbl&gt; 1.185962e-16, 1.18...
## $ pot                                         &lt;dbl&gt; -2.808814e-17, -2....
## $ dis                                         &lt;dbl&gt; 9.987197e-18, 9.98...
## $ year2009                                    &lt;dbl&gt; 0.2829368, 0.28293...
## $ year2010                                    &lt;dbl&gt; 0.1670504, 0.16705...
## $ year2011                                    &lt;dbl&gt; 0.09201842, 0.0920...
## $ year2012                                    &lt;dbl&gt; 0.1232284, 0.12322...
## $ `white_comp_wit_vict1:black_arr_susp1`      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, ...
## $ `white_comp_wit_vict1:crime_typeNuisance`   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, ...
## $ `black_arr_susp1:neighb_typeBlackDisadv`    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, ...
## $ `black_arr_susp1:neighb_typeChanging`       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, ...
## $ `crime_typeNuisance:neighb_typeBlackDisadv` &lt;dbl&gt; 0, 0, 0, 0, 0, 1, ...
## $ `crime_typeNuisance:neighb_typeChanging`    &lt;dbl&gt; 0, 0, 0, 0, 1, 0, ...
```
]

---
# Estimates!

Then we just multiply our parameters by our counterfactual data:


```r
sims_logodds &lt;- sim_params %*% t(as.matrix(x_frame))  
sims_logodds[1:6, 1:6]
```

```
##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]
## [1,] 2.093925 2.671597 2.633305 1.657692 2.321426 2.087289
## [2,] 2.211533 2.784163 2.881929 1.719465 2.419802 2.437506
## [3,] 2.259469 2.799533 2.559559 1.706017 2.399168 2.161228
## [4,] 2.058906 2.593432 2.547376 1.462574 2.050717 1.742114
## [5,] 2.094920 2.569084 2.490661 1.637049 2.226454 1.858524
## [6,] 2.173123 2.583600 2.566639 1.582291 2.201723 2.078012
```

```r
dim(sims_logodds)
```

```
## [1] 10000    24
```

Now we log-odds 10,000 estimates each (rows) of 24 counterfactual scenarios (columns).

---
# Getting Probabilities

The model for this example is a *logistic regression*, which produces estimates in *log-odds* ( `\(ln(Odds(x))\)` ).

We can convert these to probabilities based on two identities:

1. `\(Odds(x) = e^{ln(Odds(x))}\)`
2. `\(Pr(x) = \frac{Odds(x)}{(1 + Odds(x))}\)`


```r
sims_prob    &lt;- exp(sims_logodds) / (1 + exp(sims_logodds))
sims_prob[1:6, 1:6]
```

```
##           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
## [1,] 0.8903113 0.9353297 0.9329745 0.8399279 0.9106360 0.8896616
## [2,] 0.9012804 0.9418140 0.9469459 0.8480599 0.9183249 0.9196429
## [3,] 0.9054642 0.9426506 0.9282131 0.8463190 0.9167639 0.8967133
## [4,] 0.8868444 0.9304377 0.9273970 0.8119260 0.8860201 0.8509554
## [5,] 0.8904085 0.9288452 0.9234845 0.8371329 0.9026001 0.8651248
## [6,] 0.8978099 0.9297986 0.9286834 0.8295288 0.9004041 0.8887477
```

---
# A Quick Function

We are going to want to grab the mean and 95% confidence interval from our simulation estimates.

Here's a quick function to do it and make it pretty.


```r
extract_pe_ci &lt;- function(x){
  vals &lt;- c(mean(x), quantile(x, probs=c(.025, .975)))
  names(vals) &lt;- c("PE", "LB", "UB")
  return(vals)
}
```

This returns a length 3 vector with the following names:
* **PE** for *point estimate*
* **LB** for *lower bound* of the confidence interval
* **UB** for *upper bound*

---
# Prep for Plotting

First we extract our point estimates and confidence intervals by *applying* `extract_pe_ci()` to each column of estimated probabilities.

.small[

```r
estimated_pes &lt;- as.data.frame( t(apply(sims_prob, 2, extract_pe_ci)))
```
]

Then I add columns describing the scenarios to color, group, and facet over based on the counterfactual values.

.small[

```r
estimated_pes$`Reporter`     &lt;- ifelse(cf_vals[,1]==1, "Any White", "All Black")
estimated_pes$`Target`       &lt;- ifelse(cf_vals[,2]==1, "Any Black", "All White")
estimated_pes$`Crime Type`   &lt;- ifelse(cf_vals[,3]==1, "Nuisance Crime", "Serious Crime")
estimated_pes$`Neighborhood` &lt;- case_when(
  cf_vals[,4]==1 ~ "Disadvantaged",
  cf_vals[,5]==1 ~ "Changing",
  TRUE ~ "Stable White")
```
]
---
# Final Tidy Data

.small[

```r
estimated_pes %&gt;% mutate_if(is.numeric, round, digits=3) # round for display
```

```
##       PE    LB    UB  Reporter    Target     Crime Type  Neighborhood
## 1  0.894 0.877 0.909 All Black All White  Serious Crime  Stable White
## 2  0.929 0.914 0.942 All Black All White  Serious Crime      Changing
## 3  0.927 0.909 0.943 All Black All White  Serious Crime Disadvantaged
## 4  0.831 0.801 0.859 All Black All White Nuisance Crime  Stable White
## 5  0.897 0.872 0.919 All Black All White Nuisance Crime      Changing
## 6  0.879 0.845 0.909 All Black All White Nuisance Crime Disadvantaged
## 7  0.866 0.852 0.879 All Black Any Black  Serious Crime  Stable White
## 8  0.855 0.834 0.874 All Black Any Black  Serious Crime      Changing
## 9  0.850 0.824 0.874 All Black Any Black  Serious Crime Disadvantaged
## 10 0.790 0.759 0.820 All Black Any Black Nuisance Crime  Stable White
## 11 0.798 0.761 0.832 All Black Any Black Nuisance Crime      Changing
## 12 0.764 0.713 0.811 All Black Any Black Nuisance Crime Disadvantaged
## 13 0.873 0.866 0.880 Any White All White  Serious Crime  Stable White
## 14 0.914 0.900 0.927 Any White All White  Serious Crime      Changing
## 15 0.912 0.895 0.927 Any White All White  Serious Crime Disadvantaged
## 16 0.749 0.731 0.766 Any White All White Nuisance Crime  Stable White
## 17 0.841 0.812 0.868 Any White All White Nuisance Crime      Changing
## 18 0.816 0.775 0.853 Any White All White Nuisance Crime Disadvantaged
## 19 0.941 0.937 0.946 Any White Any Black  Serious Crime  Stable White
## 20 0.936 0.927 0.945 Any White Any Black  Serious Crime      Changing
## 21 0.934 0.922 0.944 Any White Any Black  Serious Crime Disadvantaged
## 22 0.875 0.863 0.885 Any White Any Black Nuisance Crime  Stable White
## 23 0.880 0.857 0.900 Any White Any Black Nuisance Crime      Changing
## 24 0.857 0.824 0.886 Any White Any Black Nuisance Crime Disadvantaged
```
]

---
# Plot Code

Finally we plot estimates (`PE`) as points with error bars (`UB`, `LB`) stratified on `Target` and `Reporter` and faceted by `Crime Type` and `Neighborhood`.

.smallish[

```r
ggplot(estimated_pes, aes(x = Target, y = PE, group = Reporter)) + 
  facet_grid(`Crime Type` ~ Neighborhood) +
  geom_errorbar(aes(ymin = LB, ymax = UB), 
                position = position_dodge(width = .4), 
                size = 0.75, width = 0.15) +
  geom_point(shape = 21, aes(fill = Reporter),
             position = position_dodge(width = .4), 
             size = 2) + 
  scale_fill_manual("Reporter", values=c("Any White" = "white", 
                                         "All Black" = "black")) +
  ggtitle("Figure 3. Probability of Arrest", 
      subtitle = "by Reporter and Target Race, Neighborhood and Crime Type") +
  xlab("Race of Target") + ylab("Estimated Probability") + 
  theme_bw() + theme(legend.position = c(0.86,0.15),
                     legend.background = element_rect(color = 1))
```
]
---
# Plot

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-33-1.svg)&lt;!-- --&gt;

---
class: inverse
# Making Tables

---
# `pander` Regression Tables

We've used `pander` to create nice tables for dataframes. But `pander` has *methods*
to handle all sort of objects that you might want displayed nicely.

This includes 
model output, such as from `lm()`, `glm()`, and `summary()`.



```r
library(pander)
```



---
# `pander()` and `lm()`

You can send an `lm()` object straight to `pander`:


```r
pander(lm_1)
```

| &amp;nbsp;          | Estimate | Std. Error | t value | Pr(&gt;t)    |
|:----------------|:--------:|:----------:|:-------:|:---------:|
| **(Intercept)** |  37.23   |   1.599    |  23.28  | 2.565e-20 |
| **wt**          |  -3.878  |   0.6327   | -6.129  | 1.12e-06  |
| **hp**          | -0.03177 |  0.00903   | -3.519  | 0.001451  |

Table: Fitting linear model: mpg ~ wt + hp

---
# `pander()` and `summary()`

You can do this with `summary()` as well, for added information:


```r
pander(summary(lm_1))
```

| &amp;nbsp;          | Estimate | Std. Error | t value | Pr(&gt;t)  |
|:----------------|:--------:|:----------:|:-------:|:---------:|
| **(Intercept)** |  37.23   |   1.599    |  23.28  | 2.565e-20 |
| **wt**          |  -3.878  |   0.6327   | -6.129  | 1.12e-06  |
| **hp**          | -0.03177 |  0.00903   | -3.519  | 0.001451  |



| Observations | Residual Std. Error | `\(R^2\)`  | Adjusted `\(R^2\)` |
|:------------:|:-------------------:|:------:|:--------------:|
|      32      |        2.593        | 0.8268 |     0.8148     |

Table: Fitting linear model: mpg ~ wt + hp

---
# `sjPlot`

`pander` tables are great for basic `rmarkdown` documents, but they're not 
generally publication ready.

The `sjPlot` package produces `html` tables that look more like
those you may find in journal articles.


```r
library(sjPlot)
```

```
## Learn more about sjPlot with 'browseVignettes("sjPlot")'.
```

---
# `sjPlot` Tables

`tab_model()` will produce tables for most models.


```r
model_1 &lt;- lm(mpg ~ wt, data = mtcars)
tab_model(model_1)
```

&lt;img src="img/sjPlot_table.PNG" width="400px" /&gt;

---
# Multi-Model Tables with `sjTable`

Often in journal articles you will see a single table that compares multiple models.

Typically, authors will start with a simple model on the left, then add variables, 
until they have their most complex model on the right.

The `sjPlot` package makes this easy to do: just give `tab_model()` more models!

---
# Multiple `tab_model()`


```r
model_2 &lt;- lm(mpg ~ hp + wt, data = mtcars)
model_3 &lt;- lm(mpg ~ hp + wt + factor(am), data = mtcars)
tab_model(model_1, model_2, model_3)
```

&lt;img src="img/sjPlot_mtable.PNG" width="1280px" /&gt;

---
# `sjPlot` does a lot more

The `sjPlot` package does *a lot* more than just make pretty tables. It is a rabbit hole
of *incredibly* powerful and useful functions for displaying descriptive and inferential results.

View the [package website](http://www.strengejacke.de/sjPlot/) for extensive documentation.

`sjPlot` is a bit more complicated than `ggeffects` but can do just about everything 
it can do as well; they were written by the same author!

`sjPlot` is fairly new but offers a fairly comprehensive solution for `ggplot`
based publication-ready social science data visualization. All graphical functions in
`sjPlot` are based on `ggplot2`, so it should not take terribly long to figure out.

---
# `sjPlot` Example: Likert plots

&lt;img src="img/sjPlot_likert.PNG" width="600px" /&gt;

---
# `sjPlot` Example: Crosstabs

&lt;img src="img/sjPlot_crosstab.PNG" width="500px" /&gt;

---
# LaTeX Tables

For tables in `\(\LaTeX\)`—as is needed for `.pdf` files—I recommend looking into the `stargazer` or `kableExtra` packages.

--

`kableExtra` allows the construction of complex tables in either HTML or `\(\LaTeX\)` using
additive syntax similar to `ggplot2`.

`stargazer` produces nicely formatted `\(\LaTeX\)` tables but is idiosyncratic.

--

If you want to edit `\(\LaTeX\)` documents, you can do it in R using Sweave documents (.Rnw).
Alternatively, you may want to work in a dedicated `\(\LaTeX\)` editor. I recommend [Overleaf](http://www.overleaf.com)
for this purpose.

--

RMarkdown has support for a fair amount of basic `\(\LaTeX\)` syntax if you aren't trying to 
get too fancy!

--

Another approach I have used is to manually format `\(\LaTeX\)` tables but use in-line R calls to 
fill in the values dynamically. This gets you the *exact* format you want but without 
forcing you to update values any time something changes.

---
# Bonus: `corrplot`

The `corrplot` package has functions for displaying correlograms.

These make visualizing the correlations between variables in a data set easier.

The first argument is a call to `cor()`, the base R function for generating a correlation matrix.

[See the vignette for customization options.](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)


```r
library(corrplot)
corrplot(
  cor(mtcars),
  addCoef.col = "white",
  addCoefasPercent=T,
  type="upper", 
  order="AOE")
```

---
## Correlogram

![](CSSS508_Week11_model_results_files/figure-html/unnamed-chunk-44-1.svg)&lt;!-- --&gt;

---
class: inverse
# Wrapping up the Course

---
# What You've Learned

A lot!

* How to get data into R from a variety of formats
* How to do "data custodian" work to manipulate and clean data
* How to make pretty visualizations
* How to automate with loops and functions
* How to combine text, calculations, plots, and tables into dynamic R Markdown reports 
* How to acquire and work with spatial data

---
# What Comes Next?

* Statistical inference (e.g. more CSSS courses)
    + Functions for hypothesis testing, hierarchical/mixed effect models, machine learning, survey design, etc. are straightforward to use... once data are clean
    + Access output by working with list structures (like from regression models) or using `broom` and `ggeffects`
* Practice, practice, practice!
    + Replicate analyses you've done in Excel, SPSS, or Stata
    + Think about data using `dplyr` verbs, tidy data principles
    + R Markdown for reproducibility
* More advanced projects
    + Using version control (git) in RStudio
    + Interactive Shiny web apps
    + Write your own functions and put them in a package
    
---
# Course Plugs

If you...

* have no stats background yet - **SOC504: Applied Social Statistics**
* want to learn more social science computing - **SOC590: Big Data and Population Processes** &lt;sup&gt;1&lt;/sup&gt;
* have (only) finished SOC506 - **CSSS510: Maximum Likelihood**
* want to master visualization - **CSSS569: Visualizing Data**
* study events or durations - **CSSS544: Event History Analysis** &lt;sup&gt;2&lt;/sup&gt;
* want to use network data - **CSSS567: Social Network Analysis**
* want to work with spatial data - **CSSS554: Spatial Statistics**

.footnote[
[1] We're hoping to offer that again soon!&lt;br&gt;
[2] Also a great maximum likelihood introduction.
]

---
class: inverse
# Thank you!
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "tomorrow-night-bright",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
